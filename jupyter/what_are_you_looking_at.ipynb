{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are sparse models looking at?\n",
    "\n",
    "This notebook provides visualizations of ResNet50 models pay attention to when they classify.  In other words, what are the models looking at.  This information can be used for localization, but here it is provided merely to build our intuition.  \n",
    "\n",
    "We use a technique from [Zhou et. al](http://cnnlocalization.csail.mit.edu/) and apply it ResNet50, building on [Alexis Cook's Keras work](https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/), we levarge the global average pooling layer that exists in ResNet50.\n",
    "\n",
    "We apply this technique to a pre-trained ResNet50 model from TorchVision (referred to as \"dense\" in the code), and to two sparse ResNet50 models trained using Distiller.  One sparse model, that we call \"sparse-fc\" is fully dense except for the last layer, which is 93% sparse.  It scores a top1 validation accuracy of 76.45 vs. the pretrained dense baseline which has a top1 score of [76.15](https://pytorch.org/docs/stable/torchvision/models.html).\n",
    "The second sparse model is 70.7% (the total sparsity of the entire model), and scores a Top1 validation accuracy of 76.09.\n",
    "\n",
    "What do these three different networks pay attention to?  Scroll down and give it a try.\n",
    "\n",
    "Unfortunately, the sparse ResNet50 models are not currently available for download from our AWS directory.  You can place the code referring to these models in remark and look at the results for dense ResNet50.\n",
    "\n",
    "See:\n",
    "* B. Zhou, A. Khosla, A. Lapedriza, A. Oliva, and A. Torralba. Learning Deep Features for Discriminative Localization. CVPR'16 (arXiv:1512.04150, 2015).\n",
    "* https://alexisbcook.github.io/2017/global-average-pooling-layers-for-object-localization/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import scipy.stats as ss\n",
    "\n",
    "# Relative import of code from distiller, w/o installing the package\n",
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import scipy   \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import distiller\n",
    "import apputils\n",
    "import models\n",
    "from apputils import *\n",
    "\n",
    "plt.style.use('seaborn') # pretty matplotlib plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imagenet_load_data(data_dir, batch_size, num_workers, shuffle=True):\n",
    "    \"\"\"Load the ImageNet dataset\"\"\"\n",
    "    test_dir = os.path.join(data_dir, 'val')\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.ImageFolder(test_dir, transforms.Compose([\n",
    "            transforms.Resize(256),\n",
    "            transforms.CenterCrop(224),\n",
    "            transforms.ToTensor(),\n",
    "            normalize,\n",
    "        ])),\n",
    "        batch_size=batch_size, shuffle=shuffle,\n",
    "        num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../../data.imagenet/val'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-200ad1cbd136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m test_loader = imagenet_load_data(\"../../data.imagenet\", \n\u001b[1;32m      5\u001b[0m                                  \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                                  num_workers=2)\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-ee2d6b15d7d9>\u001b[0m in \u001b[0;36mimagenet_load_data\u001b[0;34m(data_dir, batch_size, num_workers, shuffle)\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCenterCrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m224\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mtransforms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mToTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         ])),\n\u001b[1;32m     14\u001b[0m         \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/vnaveen0/Windows/Users/naveen/wind_drive/sfu/compression/distiller/env/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transform, target_transform, loader)\u001b[0m\n\u001b[1;32m    176\u001b[0m         super(ImageFolder, self).__init__(root, loader, IMG_EXTENSIONS,\n\u001b[1;32m    177\u001b[0m                                           \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m                                           target_transform=target_transform)\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/vnaveen0/Windows/Users/naveen/wind_drive/sfu/compression/distiller/env/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, loader, extensions, transform, target_transform)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_transform\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mclasses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0msamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_to_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextensions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/media/vnaveen0/Windows/Users/naveen/wind_drive/sfu/compression/distiller/env/lib/python3.6/site-packages/torchvision/datasets/folder.py\u001b[0m in \u001b[0;36mfind_classes\u001b[0;34m(dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_classes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mclasses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0mclasses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mclass_to_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data.imagenet/val'"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 4\n",
    "\n",
    "# Data loader\n",
    "test_loader = imagenet_load_data(\"../../data.imagenet\", \n",
    "                                 batch_size=BATCH_SIZE, \n",
    "                                 num_workers=2)\n",
    "    \n",
    "    \n",
    "# Reverse the normalization transformations we performed when we loaded the data \n",
    "# for consumption by our CNN.\n",
    "# See: https://discuss.pytorch.org/t/simple-way-to-inverse-transform-normalization/4821/3\n",
    "invTrans = transforms.Compose([ transforms.Normalize(mean = [ 0., 0., 0. ],\n",
    "                                                     std = [ 1/0.229, 1/0.224, 1/0.225 ]),\n",
    "                                transforms.Normalize(mean = [ -0.485, -0.456, -0.406 ],\n",
    "                                                     std = [ 1., 1., 1. ]),\n",
    "                               ])\n",
    "\n",
    "def process_imagenet_img(img, remove_imagenet_normalization = True):\n",
    "    if remove_imagenet_normalization:\n",
    "        img = img.clone()\n",
    "        img = invTrans(img)\n",
    "    npimg = img.numpy()\n",
    "    return np.transpose(npimg, (1, 2, 0))\n",
    "\n",
    "# Load a dictionary of {<class_id>: <class_description>}\n",
    "# Source: https://gist.github.com/yrevar/942d3a0ac09ec9e5eb3a#file-imagenet1000_clsid_to_human-txt\n",
    "from imagenet_classes import imagenet_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample some images from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly sample the training images dataset and display some images.\n",
    "# We will use these images later, but for now we just want to take a quick look at them.\n",
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "# remove the grid\n",
    "plt.grid(True)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.imshow(process_imagenet_img(torchvision.utils.make_grid(images)))\n",
    "true_class_names = [imagenet_classes[labels[j].item()] for j in range(BATCH_SIZE)]\n",
    "print('\\n'.join('%s' % class_name for class_name in true_class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load ResNet 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Load the various models\n",
    "_models = {}\n",
    "_models[\"dense\"] = models.create_model(pretrained=True, dataset='imagenet', arch='resnet50', parallel=False)\n",
    "\n",
    "# Load the FC-sparse compressed model \n",
    "_models[\"fc-sparse\"] = models.create_model(pretrained=True, dataset='imagenet', arch='resnet50', parallel=True)\n",
    "load_checkpoint(_models[\"fc-sparse\"] , \"../examples/classifier_compression/resnet50_checkpoint_93_fc-sparse_76.45-top1.pth.tar\");\n",
    "_models[\"fc-sparse\"]  = distiller.make_non_parallel_copy(_models[\"fc-sparse\"] )\n",
    "\n",
    "# Load the \"sparse\" compressed model \n",
    "_models[\"sparse\"] = models.create_model(pretrained=True, dataset='imagenet', arch='resnet50', parallel=True)\n",
    "load_checkpoint(_models[\"sparse\"] , \"../examples/classifier_compression/resnet50_checkpoint_70.66-sparse_76.09-top1.pth.tar\");\n",
    "_models[\"sparse\"]  = distiller.make_non_parallel_copy(_models[\"sparse\"] )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run a small batch of predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for the images we sampled from the dataset\n",
    "from collections import namedtuple\n",
    "Prediction = collections.namedtuple('Prediction', 'scores classids')\n",
    "\n",
    "def get_predictions(model_type):\n",
    "    model = _models[model_type]\n",
    "    model.eval()\n",
    "    predictions = model(images.cuda())\n",
    "\n",
    "    top1_vals, top1_indices = torch.max(predictions, 1)\n",
    "    print(predictions.shape)\n",
    "    #top5_vals, top5_indices = torch.max(predictions, 5)\n",
    "    predicted_class_names = [imagenet_classes[top1_indices[j].item()] for j in range(BATCH_SIZE)]\n",
    "\n",
    "    # Print inference results\n",
    "    print(\"Model: %s\" % model_type)\n",
    "    print('\\n'.join('\\t%s' % predicted_class_name for predicted_class_name in predicted_class_names)) \n",
    "    print()\n",
    "    return predictions, top1_indices, predicted_class_names\n",
    "\n",
    "predictions = {}\n",
    "top1_indices = {}\n",
    "predicted_class_names = {}\n",
    "for model_type in [\"dense\", \"sparse\", \"fc-sparse\"]:\n",
    "    predictions[model_type], top1_indices[model_type], predicted_class_names[model_type] = \\\n",
    "            get_predictions(model_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Copy the activations of the last convolution\n",
    "\n",
    "We register to the forward-hook of the last convolutional layer and copy the activations it produces.\n",
    "Because we run a forward batch of images, we also copy a batch of activations.\n",
    "\n",
    "These activations have the shape: BATCH_SIZE x 2048 x 7 x 7\n",
    "In other words, there BATCH_SIZE activations, each with 2048 channels. \n",
    "Each channel has spatial dimensions 7x7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import partial\n",
    "\n",
    "def get_last_conv_feature_maps(model, model_type):\n",
    "    def save_conv_output(m, i, o):\n",
    "        m.cached_feature_maps = o\n",
    "\n",
    "    fc_weights = model.state_dict()['fc.weight']\n",
    "    fc_layer = model.fc\n",
    "    avgpool_layer = model.avgpool   \n",
    "    last_conv = model.layer4[2].relu\n",
    "    \n",
    "    cb_handle = last_conv.register_forward_hook(save_conv_output)\n",
    "    h_x = model(images.cuda())\n",
    "    cb_handle.remove()    \n",
    "    return last_conv.cached_feature_maps\n",
    "\n",
    "last_conv_output = {}\n",
    "for model_type in [\"dense\", \"sparse\", \"fc-sparse\"]:\n",
    "    print(model_type)\n",
    "    last_conv_output[model_type] = get_last_conv_feature_maps(_models[model_type], model_type)\n",
    "    last_conv_output[model_type] = last_conv_output[model_type].detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate class activation maps (CAMs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce the class activation map for object class that is predicted to be in the image\n",
    "\n",
    "def get_CAMs(model_type):\n",
    "    CAMs = []\n",
    "    model = _models[model_type]\n",
    "    \n",
    "    for i, prediction in enumerate(top1_indices[model_type]):\n",
    "        # bilinear upsampling to resize each filtered image to size of original image \n",
    "        fc_weights = model.state_dict()['fc.weight']\n",
    "        mat_for_mult2 = scipy.ndimage.zoom(last_conv_output[model_type][i], (1, 32, 32), order=1) # dim: 224 x 224 x 2048\n",
    "\n",
    "        predicted_class = predictions[model_type]\n",
    "        CAMs.append(torch.matmul(torch.tensor(mat_for_mult2.reshape(2048, 224*224)).t().cuda(), \n",
    "                                 fc_weights[prediction].cuda()))\n",
    "    return CAMs\n",
    "        \n",
    "CAMs = {}\n",
    "for model_type in [\"dense\", \"sparse\", \"fc-sparse\"]:\n",
    "    print(\"Computing %s CAMs...\" % model_type)\n",
    "    CAMs[model_type] = get_CAMs(model_type)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display overlays of the CAMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_title(title):\n",
    "    return '\\n'.join(title.split(','))\n",
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "fontsize=15\n",
    "for i in range(BATCH_SIZE):\n",
    "    plt.subplot(3,4,i+1)\n",
    "    plt.title(\"T:\" + format_title(true_class_names[i]), fontsize=fontsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(False)\n",
    "    plt.imshow(process_imagenet_img(images[i]))\n",
    "    \n",
    "    plt.subplot(3,4,4+i+1)\n",
    "    predicted_class_name = predicted_class_names[\"dense\"][i]\n",
    "    plt.title(\"P(Dense):\" + format_title(predicted_class_name), fontsize=fontsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(False)\n",
    "    plt.imshow(process_imagenet_img(images[i]))\n",
    "    plt.imshow(CAMs[\"dense\"][i].reshape(224,224), cmap='jet', alpha=0.7)\n",
    "    \n",
    "    plt.subplot(3,4,8+i+1)\n",
    "    predicted_class_name = predicted_class_names[\"sparse\"][i]\n",
    "    plt.title(\"P(Sparse):\" + format_title(predicted_class_name), fontsize=fontsize)\n",
    "    plt.axis(\"off\")\n",
    "    plt.grid(False)\n",
    "    plt.imshow(process_imagenet_img(images[i]))\n",
    "    plt.imshow(CAMs[\"sparse\"][i].reshape(224,224), cmap='jet', alpha=0.7)\n",
    "    \n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
